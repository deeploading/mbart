# -*- coding: utf-8 -*-
"""Fine Tuning v1.5 All.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KxZey9m42CigmkzakKHSovsREsyyucw5

# 라이브러리 임포트
"""

# cuda 사용 가능한지 확인하기 
import torch

# 필요한 라이브러리들 가져오기 
import pandas as pd
from datasets import load_dataset
import evaluate
import regex as re
import transformers
import sys

metric = evaluate.load("sacrebleu") # evaluate_load()


"""# 도커에 필요한 코드"""
import argparse 

p = argparse.ArgumentParser() 
p.add_argument('--word_route', required=True)
p.add_argument('--sent_route', required=True)
p.add_argument('--dial_route', required=True)
p.add_argument('--model_route', required=True) 
p.add_argument('--model_final', required=True) 
p.add_argument('--num_train_epochs', type=int, default=3) 
p.add_argument('--batch_size', type=int, default=4) 
p.add_argument('--weight_decay', type=float, default=0.05) 
p.add_argument('--learning_rate', type=float, default=5e-5) 
config=p.parse_args() 

"""# 데이터 가져오기"""

import os
import shutil
# shutil.rmtree('../data')
try:
    os.mkdir('./data')
except Exception as e:
    print('Exception Error.', e)

# 필요한 숫자 만들기 
num_list = []
for i in range(400):
    if len(str(i)) < 2:
        num_string = "000" + str(i)
    elif len(str(i)) < 3:
        num_string = "00" + str(i)
    else:
        num_string = "0" + str(i)
    num_list.append(num_string)

slang_form, std_form = [], []

# 단어 가져오기 
import json, os, tqdm
path = config.word_route
for (dirpath, dirnames, filenames) in os.walk(path):
    for filename in filenames:
        with open(dirpath + "/" + filename,'rt', encoding='utf-8') as f:
            title = dirpath + "/" + filename
            if title.endswith(".json"):
                data = json.load(f)
                dialogs = data['Dialogs']
                for i in range(len(dialogs)):
                    if dialogs[i]['WordInfo']:   
                        slang_form.append(dialogs[i]['SpeakerText'])
                        std_form.append(dialogs[i]['TextConvert'])

df_word = pd.DataFrame()
df_word['slang'] = slang_form
df_word['standard'] = std_form 
df_word.to_csv("./data/all_words.csv")

cnt_slang = df_word['slang'].count()
print('cnt_slang',cnt_slang)

# 문장 가져오기 
slang_form, std_form = [], []
path = config.sent_route 
for (dirpath, dirnames, filenames) in os.walk(path):
    num_list_temp = num_list.copy()
    for filename in filenames:
        with open(dirpath + "/" + filename,'r', encoding='utf-8') as f:
            cut_filename = filename[18:22]
            if cut_filename in num_list_temp:
                data = json.load(f)
                dialogs = data['Dialogs']
                for i in range(len(dialogs)):
                    slang_form += [data['Dialogs'][0]['SpeakerText']]
                    std_form += [data['Dialogs'][0]['TextConvert']]
                for i in range(len(num_list_temp)):
                    if cut_filename == num_list_temp[i]:
                        del num_list_temp[i]
                        break 
            else:
                continue

import pandas as pd
df_sent = pd.DataFrame()
df_sent['slang'] = slang_form
df_sent['standard'] = std_form
df_sent.to_csv("./data/all_sent.csv")

cnt_slang = df_word['slang'].count()
print('cnt_slang',cnt_slang)

# 대화 가져오기 
slang_form, std_form = [], []
path = config.dial_route 
for (dirpath, dirnames, filenames) in os.walk(path):
    num_list_temp = num_list.copy()
    for filename in filenames:
        with open(dirpath + "/" + filename,'r', encoding='utf-8') as f:
            if len(num_list_temp) < 1:
                break
            cut_filename = filename[22:26]
            if cut_filename in num_list_temp:
                data = json.load(f)
                dialogs = data['Dialogs']
                for i in range(len(dialogs)):
                    slang_form += [dialogs[i]['SpeakerText']]
                    std_form += [dialogs[i]['TextConvert']]
                for i in range(len(num_list_temp)):
                    if cut_filename == num_list_temp[i]:
                        del num_list_temp[i]
                        break
            else:
                continue

df_conv = pd.DataFrame()
df_conv['slang'] = slang_form
df_conv['standard'] = std_form 
df_conv.to_csv("./data/all_conv.csv")

cnt_slang = df_word['slang'].count()
print('cnt_slang',cnt_slang)

# 해제는 ctrl + q 

"""# 긴 문장 자르기 & 이중전사 제거 """
data1 = pd.read_csv("./data/all_words.csv")
data2 = pd.read_csv("./data/all_sent.csv")
data3 = pd.read_csv("./data/all_conv.csv")

data = data1.append(data2, ignore_index = True)
data = data.append(data3, ignore_index = True)

# 문장 단위로 자르는 토크나이저 확인하기 
import nltk
import nltk.data
nltk.download('punkt')

sent_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')
seperated = sent_tokenizer.tokenize(data['standard'][0])

# nan인 거 드랍하기 
for i in range(len(data)):
    if type(data['standard'][i]) is float or type(data['slang'][i]) is float:
        data = data.drop(i)

data = data.reset_index()
import nltk.data
slang_form, std_form = [], []
for i in range(len(data)):
    if len(data['standard'][i]) > 120:  # 문장 길이가 길면 자르기 
        sep_slang = sent_tokenizer.tokenize(data['slang'][i])
        sep_std = sent_tokenizer.tokenize(data['standard'][i])
        if len(sep_slang) == len(sep_std) and len(sep_slang[0]) > 15:
            slang_form.extend(sep_slang)
            std_form.extend(sep_std) 
            continue
    # 문장 길이 짧은 것들 리스트에 붙이기 
    slang_form.append(data['slang'][i])
    std_form.append(data['standard'][i])

original_len = len(slang_form)
word_len = len(data1)

# 너무 짧은 문장 삭제하기 
delete_list = []
for i in range(word_len, len(slang_form)):
    if len(slang_form[i]) < 3:
        delete_list.append(i)

for i in range(len(delete_list) - 1, -1, -1):
    del slang_form[delete_list[i]]
    del std_form[delete_list[i]]

# 이중전사에서 한국어 삭제하고 숫자만 남기기 
for i in range(len(slang_form)):
    slang_sent = slang_form[i]
    if re.search('[(]', slang_sent):
        for j in range(2):
            if re.search('[(]', slang_sent):
                start = slang_sent.index("(")
                try: 
                    end = slang_sent.index(")")
                except ValueError:
                    continue
                raw_word = slang_sent[start:end + 2]
                try: 
                    start = slang_sent.index("(")
                except ValueError:
                    continue
                try:
                    end = slang_sent.index(")")
                except ValueError:
                    continue
                kor_word = slang_sent[start:end + 1] + "/"
                slang_sent = slang_sent.replace(kor_word, "")
                end = slang_sent.index(")")
                slang_sent = slang_sent[:start] + raw_word[1:-2] + slang_sent[end + 1:]
        slang_form[i] = slang_sent

# 데이터프레임으로 만들기 
data_split = pd.DataFrame()
data_split['standard'] = pd.DataFrame(std_form)
data_split['slang'] = pd.DataFrame(slang_form)

data_split

data_split.to_csv("./data/slang_dataframe_all.csv")

"""# 문장부호 없애기 """

slang_data = pd.read_csv("./data/slang_dataframe_all.csv")

length = len(slang_data)
standard = slang_data['standard']
slang = slang_data['slang']

length

# 필요없는 문장부호 제거하기 
import re 
train_standard = []
train_slang = []

for i in range(length):
    new_slang = re.sub("[^ㄱ-ㅎ ㅏ-ㅣ 가-힣 0-9 .,!?]","", slang[i])
    train_slang.append(new_slang)
    
for i in range(length):
    new_standard = re.sub("[^ㄱ-ㅎ ㅏ-ㅣ 가-힣 0-9 .,!?]","",standard[i])
    train_standard.append(new_standard)

# 문장부호 제거한 리스트 다시 데이터프레임으로 만들기 
slang_data['standard'] = train_standard
slang_data['slang'] = train_slang

slang_data[-10:]

"""# 데이터프레임 데이터셋으로 변환"""

# train 데이터 딕셔너리로 만들기 
import random as rand
train_list = []
for i in range(len(data)):
    temptemp = {}
    try:
        temptemp['standard'] = slang_data['standard'][i]
    except KeyError:
        continue
    temptemp['slang'] = slang_data['slang'][i]
    train_list.append(temptemp)

# test 데이터 딕셔너리로 만들기 
import random as rand
test_list = []
for i in range(int(0.1 * len(data))):
    index = rand.randint(0, len(data))
    temptemp = {} 
    try: 
        temptemp['standard'] = slang_data['standard'][index]
    except KeyError:
        continue 
    temptemp['slang'] = slang_data['slang'][index]
    test_list.append(temptemp)

# datasets 형식으로 만들기 
from datasets import Dataset, DatasetDict
import datasets

df_train = pd.DataFrame({"translation" : train_list})
df_test = pd.DataFrame({"translation" : test_list})

datasets = DatasetDict({
    "train": Dataset.from_pandas(df_train),
    "test" : Dataset.from_pandas(df_test)
    })

datasets

datasets['train'][0]

"""# 토크나이저 만들기"""

import re 
tok_slang, tok_std = [], []
for i in range(len(slang_data)):
    slang_sent = slang_data['slang'][i]
    std_sent = slang_data['standard'][i]
    slang_nums = re.findall(r'\d+', slang_sent)
    std_nums = re.findall(r'\d+', std_sent)
    for num in slang_nums:
        if len(num) > 1:
            num_space = " ".join(num)
            slang_sent = slang_sent.replace(num, num_space)
    for num in std_nums:
        if len(num) > 1:
            num_space = " ".join(num)
            std_sent = std_sent.replace(num, num_space)
    tok_slang.append(slang_sent)
    tok_std.append(std_sent)

tok_data = pd.DataFrame()
tok_data['slang'] = tok_slang
tok_data['standard'] = tok_std

only_sentences = tok_data[["slang", "standard"]]

len(only_sentences)

# 토크나이저 만들 파일 
only_sentences.to_csv("./data/slang_sentences.csv")

import os
import transformers
from tokenizers import BertWordPieceTokenizer

tokenizer = BertWordPieceTokenizer(strip_accents=False, lowercase=False, clean_text=True, wordpieces_prefix="##")  # 악센트 제거 x, 대소문자 구분 x

corpus_file   = './data/slang_sentences.csv'
vocab_size    = 32000
limit_alphabet= 60000 
min_frequency = 3 

# 데이터를 학습하여 단어 집합 얻기 
tokenizer.train(files=corpus_file,  
               vocab_size=vocab_size,  # 단어 집합의 크기 
               min_frequency=min_frequency,  # 최소 해당 횟수만큼 등장한 쌍의 경우 병합 대상
               limit_alphabet=limit_alphabet,  # 병합 전 초기 토큰 허용 개수 
               show_progress=True)

# shutil.rmtree('../tokenizer')
try:
    os.mkdir('./tokenizer')
except Exception as e:
    print('Exception Error.', e)

tokenizer.save_model('./tokenizer')

from transformers import BertTokenizerFast

# 단어 집합을 사용해서 토크나이징을 하는 토크나이저 
tokenizer = BertTokenizerFast.from_pretrained('./tokenizer', strip_accents=False, lowercase=False) 

tokenized_input_for_pytorch = tokenizer(slang_data['standard'][0], truncation=True, # 문장 너무 길면 자르기
                                        return_tensors="pt", # pytorch 형태로 반환
                                        max_length=50,  # 50개 맞춰 패딩
                                        padding=True)  

max_input_length = 50
max_target_length = 50

def preprocess_function(samples):  
    inputs = [s["slang"] for s in samples["translation"]]  # 입력은 은어 속어가 포함된 문장
    targets = [s["standard"] for s in samples["translation"]]  # 라벨은 표준어로 번역된 문장 
    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)  # 토크나이징 된 모델 입력 

    with tokenizer.as_target_tokenizer():
        labels = tokenizer(targets, max_length=max_target_length, truncation=True)  # 토크나이징 된 모델 라벨 

    model_inputs["labels"] = labels["input_ids"]  # 라벨을 model_inputs에 추가 
    return model_inputs

# input_ids는 입력 토큰
# token_type_ids는 시퀀스를 구분해줌 
# attention_mask는 padding 부분을 제외하기 위한 mask
# labels는 라벨 토큰 
preprocess_function(datasets['train'][:1])

# 모든 데이터 토크나이징 하기 
tokenized_datasets = datasets.map(preprocess_function, batched=True)

tokenized_datasets

tokenized_datasets = tokenized_datasets.remove_columns(["token_type_ids"])

"""# 모델 파인 튜닝하기"""

from transformers import MBartForConditionalGeneration # 언어 모델 헤드를 가진 BART 모델 
from transformers import DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer
#model = MBartForConditionalGeneration.from_pretrained(config.model_route)
model = MBartForConditionalGeneration.from_pretrained("facebook/mbart-large-cc25")
batch_size = 4
args = Seq2SeqTrainingArguments(  # 학습 시 사용되는 인자들 
    config.model_final,
    evaluation_strategy = 'epoch',  # epoch 마다 evaluation 수행 
    overwrite_output_dir = 'True',
    learning_rate=config.learning_rate,  # 학습률, 너무 작으면 overfitting 됨 
    num_train_epochs=config.num_train_epochs,  # epoch 수 
    per_device_train_batch_size=config.batch_size,  # train batch 사이즈
    per_device_eval_batch_size=config.batch_size,  # evaluation batch 사이즈
    weight_decay=config.weight_decay,  # 클수록 가중치 값이 작아져서 오버피팅을 해소할 수 있음 
    save_total_limit=1,  # 저장 횟수 
    predict_with_generate=True,  # 학습할 때 결과 생성함 
)

from transformers import DataCollatorForSeq2Seq  # 입력과 라벨에 패딩을 해서 배치 형성시켜줌 
data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)

import numpy as np

def postprocess_text(preds, labels):  # 문장 뒤에 스페이스 없애는 함수  
    preds = [pred.strip() for pred in preds]
    labels = [[label.strip()] for label in labels]

    return preds, labels

def compute_metrics(eval_preds):  # epoch 끝날 때마다 BLEU 스코어와 결과 길이 측정하는 함수 
    preds, labels = eval_preds
    if isinstance(preds, tuple):
        preds = preds[0]
    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)  # 예측값 토큰을 글자로 바꿔줌 

    # Replace -100 in the labels as we can't decode them.
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)  # 라벨 토큰을 글자로 바꿔줌 

    # Some simple post-processing
    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)  # 스페이스 제거 

    result = metric.compute(predictions=decoded_preds, references=decoded_labels)  # BLEU 스코어 계산
    result = {"bleu": result["score"]}

    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]  # 생성 문자 길이 계산
    result["gen_len"] = np.mean(prediction_lens)  # 평균 내기 
    result = {k: round(v, 4) for k, v in result.items()}
    return result

trainer = Seq2SeqTrainer(  # feature-complete training and eval loop
    model,  # 학습 모델 
    args,  # 학습 시 사용되는 인자들 
    train_dataset=tokenized_datasets["train"], 
    eval_dataset=tokenized_datasets["test"],
    data_collator=data_collator,  
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)

print('Begin Train...')
trainer.train()
print('Begin Save Model...')
trainer.save_model(config.model_final)
print('Train End...')